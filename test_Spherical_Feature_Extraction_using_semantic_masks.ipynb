{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical Feature Extraction using Spherical_VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import BatchNorm2d\n",
    "import torch.nn.functional as F\n",
    "from SphereNet_PyTorch.spherenet import SphereConv2D, SphereMaxPool2D\n",
    "\"\"\"\n",
    "class SphereNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SphereNet, self).__init__()\n",
    "        self.conv1 = SphereConv2D(3, 32, stride=1)\n",
    "        self.pool1 = SphereMaxPool2D(stride=2)\n",
    "        self.conv2 = SphereConv2D(32, 64, stride=1)\n",
    "        self.pool2 = SphereMaxPool2D(stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool1(self.conv1(x)))\n",
    "        x = F.relu(self.pool2(self.conv2(x)))\n",
    "        return x\n",
    "\"\"\"\n",
    "class SphereNetVGG16(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SphereNetVGG16, self).__init__()\n",
    "        # conv_block_1\n",
    "        self.conv1_1 = SphereConv2D(3 , 64, stride=1)\n",
    "        self.conv1_2 = SphereConv2D(64, 64, stride=1)\n",
    "        # conv_block_2\n",
    "        self.conv2_1 = SphereConv2D(64 , 128, stride=1)\n",
    "        self.conv2_2 = SphereConv2D(128, 128, stride=1)\n",
    "        # conv_block_3\n",
    "        self.conv3_1 = SphereConv2D(128, 256, stride=1)\n",
    "        self.conv3_2 = SphereConv2D(256, 256, stride=1)\n",
    "        self.conv3_3 = SphereConv2D(256, 256, stride=1)\n",
    "        # conv_block_4\n",
    "        self.conv4_1 = SphereConv2D(256, 512, stride=1)\n",
    "        self.conv4_2 = SphereConv2D(512, 512, stride=1)\n",
    "        self.conv4_3 = SphereConv2D(512, 512, stride=1)\n",
    "        # conv_block_5\n",
    "        self.conv5_1 = SphereConv2D(512, 512, stride=1)\n",
    "        self.conv5_2 = SphereConv2D(512, 512, stride=1)\n",
    "        self.conv5_3 = SphereConv2D(512, 512, stride=1)\n",
    "        self.pool = SphereMaxPool2D(stride=2)\n",
    "    \"\"\"\n",
    "        self.bn1  = BatchNorm2d(64)\n",
    "        self.bn2  = BatchNorm2d(128)\n",
    "        self.bn3  = BatchNorm2d(256)\n",
    "        self.bn4  = BatchNorm2d(512)\n",
    "    \n",
    "    # BN added VGG16\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1_1(x)))\n",
    "        x = F.relu(self.bn1(self.conv1_2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2_1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2_2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3_1(x)))\n",
    "        x = F.relu(self.bn3(self.conv3_2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3_3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4_1(x)))\n",
    "        x = F.relu(self.bn4(self.conv4_2(x)))\n",
    "        x = F.relu(self.bn4(self.conv4_3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv5_1(x)))\n",
    "        x = F.relu(self.bn4(self.conv5_2(x)))\n",
    "        x = F.relu(self.bn4(self.conv5_3(x)))\n",
    "        x = self.pool(x)\n",
    "        #x = F.relu(x)\n",
    "        return x\n",
    "    \"\"\"\n",
    "    # Pure VGG16\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_2(F.relu(self.conv1_1(x))))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2_2(F.relu(self.conv2_1(x))))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3_3(F.relu(self.conv3_2(F.relu(self.conv3_1(x))))))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4_3(F.relu(self.conv4_2(F.relu(self.conv4_1(x))))))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5_3(F.relu(self.conv5_2(F.relu(self.conv5_1(x))))))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, SphereConv2D):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndevice = torch.device('cpu')\\nspheremodel = SphereNetVGG16().to(device)\\n# Retrieve SphereNet model state\\nsphere_dict = spheremodel.state_dict()\\n\\nfor k, v in sphere_dict.items():\\n    print(k,'-',v.shape)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "device = torch.device('cpu')\n",
    "spheremodel = SphereNetVGG16().to(device)\n",
    "# Retrieve SphereNet model state\n",
    "sphere_dict = spheremodel.state_dict()\n",
    "\n",
    "for k, v in sphere_dict.items():\n",
    "    print(k,'-',v.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load pre-trained VGG16 model state\\nvgg16_dict = torch.load('C:/Users/ustundag/.cache/torch/checkpoints/vgg16-397923af.pth')\\n# Cut out the classifier layers at the end\\nvgg16_dict = {k: vgg16_dict[k] for k in vgg16_dict.keys() if 'classifier' not in k}\\n\\n# Assign VGG16 weights to the layers of SphereNet\\nfor (k_sph,v_sph), (k_vgg,v_vgg) in zip(sphere_dict.items(), vgg16_dict.items()):\\n    sphere_dict[k_sph] = v_vgg\\n\\nfor k, v in sphere_dict.items():\\n    print(k,'-',v.shape)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Load pre-trained VGG16 model state\n",
    "vgg16_dict = torch.load('C:/Users/ustundag/.cache/torch/checkpoints/vgg16-397923af.pth')\n",
    "# Cut out the classifier layers at the end\n",
    "vgg16_dict = {k: vgg16_dict[k] for k in vgg16_dict.keys() if 'classifier' not in k}\n",
    "\n",
    "# Assign VGG16 weights to the layers of SphereNet\n",
    "for (k_sph,v_sph), (k_vgg,v_vgg) in zip(sphere_dict.items(), vgg16_dict.items()):\n",
    "    sphere_dict[k_sph] = v_vgg\n",
    "\n",
    "for k, v in sphere_dict.items():\n",
    "    print(k,'-',v.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport torchvision.transforms.functional as tfun\\n\\npath = 'pano_rgb.png'\\nimg = Image.open(path)\\nimg = img.resize((512,256))\\ndata = np.asarray(img, dtype=np.float32)\\ndata = tfun.to_tensor(data)\\ndata = data.unsqueeze_(0)\\ndata = data[:,:3,:,:]\\nprint(data.shape)\\nplt.imshow(img)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torchvision.transforms.functional as tfun\n",
    "\n",
    "path = 'pano_rgb.png'\n",
    "img = Image.open(path)\n",
    "img = img.resize((512,256))\n",
    "data = np.asarray(img, dtype=np.float32)\n",
    "data = tfun.to_tensor(data)\n",
    "data = data.unsqueeze_(0)\n",
    "data = data[:,:3,:,:]\n",
    "print(data.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfmap = spheremodel(data.to(device))\\n#fmap = fmap.reshape(fmap.shape[0], fmap.shape[1], fmap.shape[2]*fmap.shape[3])\\nprint(fmap.shape)\\n\\nx = fmap.detach().numpy()\\nplt.figure(figsize=(8, 8))\\nfor i in range(2):\\n    plt.subplot(4, 1, i + 1)\\n    plt.imshow(x[0,i,:,:])\\nplt.show()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fmap = spheremodel(data.to(device))\n",
    "#fmap = fmap.reshape(fmap.shape[0], fmap.shape[1], fmap.shape[2]*fmap.shape[3])\n",
    "print(fmap.shape)\n",
    "\n",
    "x = fmap.detach().numpy()\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(2):\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    plt.imshow(x[0,i,:,:])\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npath = 'pano_semantic.png'\\n# The more scale down, the less number of unique pixels!\\nimg = np.asarray(Image.open(path).resize((16,8)))\\nimg_pixels    = img.reshape(img.shape[0]*img.shape[1], img.shape[2])\\nvalid_indexes = [[np.argwhere((img_pixels == p).all(axis=1))[0,0], get_label(p)]\\n                    for p in img_pixels\\n                    if get_label(p) in VALID_OBJECTS]\\nvalid_indexes\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = 'pano_semantic.png'\n",
    "# The more scale down, the less number of unique pixels!\n",
    "img = np.asarray(Image.open(path).resize((16,8)))\n",
    "img_pixels    = img.reshape(img.shape[0]*img.shape[1], img.shape[2])\n",
    "valid_indexes = [[np.argwhere((img_pixels == p).all(axis=1))[0,0], get_label(p)]\n",
    "                    for p in img_pixels\n",
    "                    if get_label(p) in VALID_OBJECTS]\n",
    "valid_indexes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create chair masks using semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assets.utils as u\n",
    "VALID_OBJECTS = ('board','bookcase','chair','door','sofa','table','window')\n",
    "\n",
    "import glob\n",
    "from scipy.io import savemat, loadmat\n",
    "from IPython.display import display, clear_output\n",
    "import torchvision.transforms.functional as tfun\n",
    "\n",
    "def get_label(pix):\n",
    "    labels = u.load_labels('C:/Users/ustundag/Github/2D-3D-Semantics/assets/semantic_labels.json')\n",
    "    limit = len(labels)\n",
    "    i = u.get_index(pix)\n",
    "    if i < limit:\n",
    "        instance_label = labels[i]\n",
    "        instance_label_as_dict = u.parse_label(instance_label)\n",
    "        label = instance_label_as_dict[\"instance_class\"]\n",
    "        return label\n",
    "    return '<UNK>' # unknown in case index is out of bounds in \"labels.json\" file\n",
    "\n",
    "def image2tensor(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((512,256))\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    tensor = tfun.to_tensor(img)\n",
    "    tensor = tensor[:3,:,:]\n",
    "    tensor = tensor.unsqueeze_(0)\n",
    "    return tensor\n",
    "\n",
    "def create_trained_model(device):\n",
    "    sphereNet = SphereNetVGG16().to(device)\n",
    "    # Retrieve SphereNet model state\n",
    "    sphereNet_state = sphereNet.state_dict()\n",
    "    \n",
    "    # Load pre-trained VGG16 model state\n",
    "    vgg16_dict = torch.load('C:/Users/ustundag/.cache/torch/checkpoints/vgg16-397923af.pth')\n",
    "    # Cut out the classifier layers at the end\n",
    "    vgg16_dict = {k: vgg16_dict[k] for k in vgg16_dict.keys() if 'classifier' not in k}\n",
    "\n",
    "    # Assign VGG16 weights to the layers of SphereNet\n",
    "    for (k_sph,v_sph), (k_vgg,v_vgg) in zip(sphereNet_state.items(), vgg16_dict.items()):\n",
    "        sphereNet_state[k_sph] = v_vgg\n",
    "    \n",
    "    sphereNet.load_state_dict(sphereNet_state)\n",
    "    return sphereNet\n",
    "\n",
    "def save_features_and_labels(file):\n",
    "    paths = glob.glob(\"C:\\\\Users\\\\ustundag\\\\GitHub\\\\2D-3D-Semantics\\\\area_3\\\\pano\\\\rgb\\\\*.png\")\n",
    "    features = []\n",
    "    labels = []\n",
    "    device = torch.device('cpu')\n",
    "    sphereNet = create_trained_model(device)\n",
    "    i = 1\n",
    "    for path in paths:\n",
    "        clear_output(wait=True)\n",
    "        tensor = image2tensor(path)\n",
    "        fmap = sphereNet(tensor.to(device))\n",
    "        fmap = fmap.detach().numpy()\n",
    "        fmap = fmap.reshape(fmap.shape[0], fmap.shape[1], fmap.shape[2]*fmap.shape[3])\n",
    "\n",
    "        # Replace 2 occurrences to find counterpart of RGB image as Semantic\n",
    "        sem_file = path.replace(\"rgb\", \"semantic\", 2)\n",
    "        sem_img  = np.asarray(Image.open(sem_file).resize((16,8)))\n",
    "        sem_pixels = sem_img.reshape(sem_img.shape[0]*sem_img.shape[1], sem_img.shape[2])\n",
    "        valid_indexes = [[np.argwhere((sem_pixels == p).all(axis=1))[0,0], get_label(p)]\n",
    "                            for p in sem_pixels\n",
    "                            if get_label(p) in VALID_OBJECTS]\n",
    "        # first value = feature index, second value = label\n",
    "        for idx in valid_indexes:\n",
    "            features.append(fmap[0,:,idx[0]])\n",
    "            labels.append(VALID_OBJECTS.index(idx[1]))\n",
    "\n",
    "        display(str(i) + \" / 85\")\n",
    "        i += 1\n",
    "    \n",
    "    savemat(file,{'features': np.asarray(features),\n",
    "                  'labels'  : np.asarray(labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85 / 85'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = 'area_3_data_pano_spherenet_VGG16_dims_512_256_16_8_batchNorm_no_relu_after_onlyConv_weights_added.mat'\n",
    "save_features_and_labels(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"C:\\\\Users\\\\ustundag\\\\GitHub\\\\2D-3D-Semantics\\\\\"+file)\n",
    "features = data[\"features\"]\n",
    "labels   = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1530, 512)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1530)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
