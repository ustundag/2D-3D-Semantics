{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import keras\n",
    "import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import keras.backend as K\n",
    "import glob\n",
    "from scipy.io import loadmat \n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, UpSampling2D, Activation\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = loadmat(\"C:\\\\Users\\\\ustundag\\\\GitHub\\\\2D-3D-Semantics\\\\noXYZ_area_3_no_xyz_data_semantic_90x90.mat\")\n",
    "images = images[\"semantic\"]\n",
    "labels = loadmat(\"C:\\\\Users\\\\ustundag\\\\GitHub\\\\2D-3D-Semantics\\\\noXYZ_area_3_no_xyz_data_semantic_90x90_labels.mat\")\n",
    "labels = labels[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3704, 8100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign ground truth labels\n",
    "labels_gt = labels[0]\n",
    "# Split dataset into tarin and test\n",
    "x_train = images[:3000] / 255.0\n",
    "x_test  = images[-704:] / 255.0\n",
    "y_train = labels_gt[:3000]\n",
    "y_test  = labels_gt[-704:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_room_type(label):\n",
    "    if label == 0: return 'WC'\n",
    "    if label == 1: return 'conferenceRoom'\n",
    "    if label == 2: return 'hallway'\n",
    "    if label == 3: return 'lounge'\n",
    "    if label == 4: return 'office'\n",
    "    if label == 5: return 'storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room type: office\n"
     ]
    }
   ],
   "source": [
    "i = 1234\n",
    "pylab.imshow(x_train[i].reshape(90, 90), cmap='gray')\n",
    "pylab.show()\n",
    "print('Room type: ' + get_room_type(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Beasic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_jobs=-1, n_clusters = 6, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=6, n_init=20, n_jobs=-1, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = km.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10249780416897661"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "normalized_mutual_info_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder + KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python368-64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8100)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               4050500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                60030     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2000)              62000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8100)              4058100   \n",
      "=================================================================\n",
      "Total params: 10,734,130\n",
      "Trainable params: 10,734,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this is our input placeholder\n",
    "input_img = Input(shape=(8100,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(30, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(8100)(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python368-64\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3000 samples, validate on 704 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.5535e-06 - val_loss: 2.9046e-08\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0172e-08 - val_loss: 2.8800e-08\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0237e-08 - val_loss: 2.9024e-08\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0293e-08 - val_loss: 2.8898e-08\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0319e-08 - val_loss: 2.9022e-08\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0378e-08 - val_loss: 2.8886e-08\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0409e-08 - val_loss: 2.8978e-08\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0534e-08 - val_loss: 2.9047e-08\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0431e-08 - val_loss: 2.8801e-08\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0514e-08 - val_loss: 2.9027e-08\n"
     ]
    }
   ],
   "source": [
    "train_history = autoencoder.fit(x_train, x_train,\n",
    "                                epochs=10,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auto_train = encoder.predict(x_train)\n",
    "pred_auto = encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(pred_auto_train)\n",
    "pred = km.predict(pred_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09322916215015713"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvAutoencoder + KMeans (currently not working, in progress...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images\n",
    "x_train_s = x_train.reshape(-1,90,90,1)\n",
    "x_test_s  = x_test.reshape(-1,90,90,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the autoencoder\n",
    "model = Sequential()\n",
    "model.add(Conv2D(45, kernel_size=3, padding='same', activation='relu', input_shape=(90,90,1)))\n",
    "model.add(MaxPool2D((3,3), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(15, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D((3,3), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(15, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(UpSampling2D((3,3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(45, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(UpSampling2D((3,3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(1, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train_s, x_train_s, epochs=10, batch_size=64, validation_data=(x_test_s, x_test_s), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting testing dataset\n",
    "restored_testing_dataset = model.predict(x_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the reconstructed image quality\n",
    "plt.figure(figsize=(20,5))\n",
    "for i in range(5):\n",
    "    index = y_test.tolist().index(i)\n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.imshow(x_test_s[index].reshape((90,90)), cmap='gray')\n",
    "    plt.gray()\n",
    "    plt.subplot(2, 6, i+7)\n",
    "    plt.imshow(restored_testing_dataset[index].reshape((90,90)), cmap='gray')\n",
    "    plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the encoder\n",
    "encoder = K.function([model.layers[0].input], [model.layers[4].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training set\n",
    "encoded_images = encoder([x_test_s])[0].reshape(-1, 10*10*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the training set\n",
    "kmeans = KMeans(n_clusters = 6)\n",
    "clustered_training_set = kmeans.fit_predict(encoded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe and compare clustering result with actual label using confusion matrix\n",
    "cm = confusion_matrix(y_test, clustered_training_set)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\", fontsize=20)\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Clustering label', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual pictures grouped by clustering\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for r in range(6):\n",
    "    cluster = cm[r].argmax()\n",
    "    for c, val in enumerate(x_test_s[clustered_training_set == cluster][0:6]):\n",
    "        fig.add_subplot(6, 6, 6*r+c+1)\n",
    "        plt.imshow(val.reshape((90,90)))\n",
    "        plt.gray()\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel('cluster: '+str(cluster))\n",
    "        plt.ylabel('digit: '+str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mutual_info_score(y_test, clustered_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Embedded Clustering (DEC) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "Original Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=6))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=6,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.acc(self.y, y_pred), metrics.nmi(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/50\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 3.9507e-08\n",
      "        |==>  acc: 0.2967,  nmi: 0.0597  <==|\n",
      "Epoch 2/50\n",
      "  64/3000 [..............................] - ETA: 7s - loss: 2.9055e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n",
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0445e-08\n",
      "Epoch 3/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0401e-08\n",
      "Epoch 4/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0723e-08\n",
      "Epoch 5/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0595e-08\n",
      "Epoch 6/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0635e-08\n",
      "        |==>  acc: 0.2983,  nmi: 0.0841  <==|\n",
      "Epoch 7/50\n",
      "  64/3000 [..............................] - ETA: 7s - loss: 3.0790e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0825e-08\n",
      "Epoch 8/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0780e-08\n",
      "Epoch 9/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0871e-08\n",
      "Epoch 10/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0877e-08\n",
      "Epoch 11/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0748e-08\n",
      "        |==>  acc: 0.3137,  nmi: 0.0947  <==|\n",
      "Epoch 12/50\n",
      "  64/3000 [..............................] - ETA: 6s - loss: 2.7333e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0831e-08\n",
      "Epoch 13/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0875e-08\n",
      "Epoch 14/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0894e-08\n",
      "Epoch 15/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0969e-08\n",
      "Epoch 16/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0907e-08\n",
      "        |==>  acc: 0.3313,  nmi: 0.1041  <==|\n",
      "Epoch 17/50\n",
      "  64/3000 [..............................] - ETA: 6s - loss: 3.1664e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0909e-08\n",
      "Epoch 18/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0918e-08\n",
      "Epoch 19/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0862e-08\n",
      "Epoch 20/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0940e-08\n",
      "Epoch 21/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0867e-08\n",
      "        |==>  acc: 0.3337,  nmi: 0.1064  <==|\n",
      "Epoch 22/50\n",
      "  64/3000 [..............................] - ETA: 7s - loss: 2.8539e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1055e-08\n",
      "Epoch 23/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0975e-08\n",
      "Epoch 24/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1143e-08\n",
      "Epoch 25/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0867e-08\n",
      "Epoch 26/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0809e-08\n",
      "        |==>  acc: 0.3363,  nmi: 0.1070  <==|\n",
      "Epoch 27/50\n",
      "  64/3000 [..............................] - ETA: 7s - loss: 3.3621e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0997e-08\n",
      "Epoch 28/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1132e-08\n",
      "Epoch 29/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1201e-08\n",
      "Epoch 30/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1061e-08\n",
      "Epoch 31/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1130e-08\n",
      "        |==>  acc: 0.3337,  nmi: 0.1068  <==|\n",
      "Epoch 32/50\n",
      "  64/3000 [..............................] - ETA: 6s - loss: 2.9165e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1021e-08\n",
      "Epoch 33/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1057e-08\n",
      "Epoch 34/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0966e-08\n",
      "Epoch 35/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1179e-08\n",
      "Epoch 36/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0988e-08\n",
      "        |==>  acc: 0.3337,  nmi: 0.1065  <==|\n",
      "Epoch 37/50\n",
      "  64/3000 [..............................] - ETA: 6s - loss: 3.0946e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1172e-08\n",
      "Epoch 38/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1002e-08\n",
      "Epoch 39/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1213e-08\n",
      "Epoch 40/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1081e-08\n",
      "Epoch 41/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1022e-08\n",
      "        |==>  acc: 0.3357,  nmi: 0.1075  <==|\n",
      "Epoch 42/50\n",
      "  64/3000 [..............................] - ETA: 6s - loss: 3.1962e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1022e-08\n",
      "Epoch 43/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1094e-08\n",
      "Epoch 44/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0915e-08\n",
      "Epoch 45/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.1028e-08\n",
      "Epoch 46/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0908e-08\n",
      "        |==>  acc: 0.3353,  nmi: 0.1083  <==|\n",
      "Epoch 47/50\n",
      "  64/3000 [..............................] - ETA: 6s - loss: 2.9792e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0937e-08\n",
      "Epoch 48/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0967e-08\n",
      "Epoch 49/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0816e-08\n",
      "Epoch 50/50\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 3.0952e-08\n",
      "Pretraining time:  362.979736328125\n",
      "Pretrained weights are saved to results/ae_weights.h5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Deep_Embedding_Clustering')\n",
    "from Deep_Embedding_Clustering import metrics\n",
    "\n",
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 32\n",
    "maxiter = 2e4\n",
    "tol = 0.001\n",
    "save_dir = 'results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 200\n",
    "pretrain_epochs = 50\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "#pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "dec = DEC(dims=[x_train.shape[-1], 500, 500, 2000, 10], n_clusters=6, init=init)\n",
    "\n",
    "dec.pretrain(x=x_train, y=y_train, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 8100)              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               4050500   \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 6)                 60        \n",
      "=================================================================\n",
      "Total params: 5,323,070\n",
      "Trainable params: 5,323,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 200\n",
      "Save interval 468.75\n",
      "Initializing cluster centers with k-means.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.36867, nmi = 0.07789, ari = 0.00981  ; loss= 0\n",
      "saving model to: results/DEC_model_0.h5\n",
      "Iter 200: acc = 0.36867, nmi = 0.07789, ari = 0.00981  ; loss= 0\n",
      "delta_label  0.0 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: results/DEC_model_final.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python368-64\\lib\\site-packages\\sklearn\\utils\\linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec.fit(x_train, y=y_train, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = dec.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07305634730107706"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_test, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
