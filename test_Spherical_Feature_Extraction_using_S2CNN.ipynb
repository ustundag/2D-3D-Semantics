{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical Feature Extraction using s2cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/ustundag/GitHub/2D-3D-Semantics/s2cnn_TORCH/')\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.transforms.functional as tfun\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from s2cnn import SO3Convolution\n",
    "from s2cnn import S2Convolution\n",
    "from s2cnn import so3_integrate\n",
    "from s2cnn import so3_near_identity_grid\n",
    "from s2cnn import s2_near_identity_grid\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2ConvNet_deep(nn.Module):\n",
    "    def __init__(self, bandwidth = 30):\n",
    "        super(S2ConvNet_deep, self).__init__()\n",
    "        grid_s2    =  s2_near_identity_grid(n_alpha=6, max_beta=np.pi/16, n_beta=1)\n",
    "        grid_so3_1 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/16, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_2 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/8,  n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_3 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/4,  n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_4 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/2,  n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_5 = so3_near_identity_grid(n_alpha=6, max_beta=0.2,      n_beta=1)\n",
    "\n",
    "        self.convolutional = nn.Sequential(\n",
    "            S2Convolution(\n",
    "                nfeature_in  = 3,\n",
    "                nfeature_out = 8,\n",
    "                b_in  = bandwidth,\n",
    "                b_out = bandwidth,\n",
    "                grid=grid_s2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  =  8,\n",
    "                nfeature_out = 16,\n",
    "                b_in  = bandwidth,\n",
    "                b_out = bandwidth//2,\n",
    "                grid=grid_so3_1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 16,\n",
    "                nfeature_out = 16,\n",
    "                b_in  = bandwidth//2,\n",
    "                b_out = bandwidth//2,\n",
    "                grid=grid_so3_2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 16,\n",
    "                nfeature_out = 24,\n",
    "                b_in  = bandwidth//2,\n",
    "                b_out = bandwidth//4,\n",
    "                grid=grid_so3_2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 24,\n",
    "                nfeature_out = 24,\n",
    "                b_in  = bandwidth//4,\n",
    "                b_out = bandwidth//4,\n",
    "                grid=grid_so3_3),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 24,\n",
    "                nfeature_out = 32,\n",
    "                b_in  = bandwidth//4,\n",
    "                b_out = bandwidth//8,\n",
    "                grid=grid_so3_3),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 32,\n",
    "                nfeature_out = 64,\n",
    "                b_in  = bandwidth//8,\n",
    "                b_out = bandwidth//8,\n",
    "                grid=grid_so3_4),\n",
    "            nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        #x = so3_integrate(x)\n",
    "        #x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2ConvNet_deep(\n",
       "  (convolutional): Sequential(\n",
       "    (0): S2Convolution()\n",
       "    (1): ReLU()\n",
       "    (2): SO3Convolution()\n",
       "    (3): ReLU()\n",
       "    (4): SO3Convolution()\n",
       "    (5): ReLU()\n",
       "    (6): SO3Convolution()\n",
       "    (7): ReLU()\n",
       "    (8): SO3Convolution()\n",
       "    (9): ReLU()\n",
       "    (10): SO3Convolution()\n",
       "    (11): ReLU()\n",
       "    (12): SO3Convolution()\n",
       "    (13): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2cnn = S2ConvNet_deep(bandwidth=64)\n",
    "s2cnn.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npath = 'C:/Users/ustundag/Desktop/test_pano_rgb.png'\\nimg = Image.open(path)\\nimg = img.resize((128,128))\\ndata = np.asarray(img, dtype=np.float32)\\ndata = tfun.to_tensor(data)\\ndata = data.unsqueeze_(0)\\ndata = data[:,:3,:,:]\\nprint(data.shape)\\nplt.imshow(img)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = 'C:/Users/ustundag/Desktop/test_pano_rgb.png'\n",
    "img = Image.open(path)\n",
    "img = img.resize((128,128))\n",
    "data = np.asarray(img, dtype=np.float32)\n",
    "data = tfun.to_tensor(data)\n",
    "data = data.unsqueeze_(0)\n",
    "data = data[:,:3,:,:]\n",
    "print(data.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimages  = data.to(DEVICE)\\noutputs = s2cnn(images)\\nprint('outputs.shape: ', outputs.shape)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "images  = data.to(DEVICE)\n",
    "outputs = s2cnn(images)\n",
    "print('outputs.shape: ', outputs.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx = outputs.detach().cpu().numpy()\\na = x[0, 0, :, :, 10]\\nprint(a.shape)\\nplt.imshow(a, cmap='gray')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = outputs.detach().cpu().numpy()\n",
    "a = x[0, 0, :, :, 10]\n",
    "print(a.shape)\n",
    "plt.imshow(a, cmap='gray')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and save features of 7 specific objects using semantics as masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assets.utils as u\n",
    "VALID_OBJECTS = ('board','bookcase','chair','door','sofa','table','window')\n",
    "\n",
    "import glob\n",
    "from scipy.io import savemat, loadmat\n",
    "from IPython.display import display, clear_output\n",
    "import torchvision.transforms.functional as tfun\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_label(pix):\n",
    "    labels = u.load_labels('C:/Users/ustundag/Github/2D-3D-Semantics/assets/semantic_labels.json')\n",
    "    limit = len(labels)\n",
    "    i = u.get_index(pix)\n",
    "    if i < limit:\n",
    "        instance_label = labels[i]\n",
    "        instance_label_as_dict = u.parse_label(instance_label)\n",
    "        label = instance_label_as_dict[\"instance_class\"]\n",
    "        return label\n",
    "    return '<UNK>' # unknown in case index is out of bounds in \"labels.json\" file\n",
    "\n",
    "def image2tensor(path, dim):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((dim,dim))\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    tensor = tfun.to_tensor(img)\n",
    "    tensor = tensor[:3,:,:]\n",
    "    tensor = tensor.unsqueeze_(0)\n",
    "    return tensor\n",
    "\n",
    "def save_features_and_labels(file):\n",
    "    paths = glob.glob(\"C:\\\\Users\\\\ustundag\\\\GitHub\\\\2D-3D-Semantics\\\\area_3\\\\pano\\\\rgb\\\\*.png\")\n",
    "    features = []\n",
    "    labels = []\n",
    "    s2cnn = S2ConvNet_deep(bandwidth=64)\n",
    "    s2cnn.to(DEVICE)\n",
    "    \n",
    "    i = 1\n",
    "    for path in paths:\n",
    "        clear_output(wait=True)\n",
    "        tensor = image2tensor(path, dim=128) # 'dim' must be double of bandwidth\n",
    "        images = tensor.to(DEVICE)\n",
    "        fmap = s2cnn(images) # torch.Size([1, 64, 16, 16, 16])\n",
    "        fmap = fmap.detach().cpu().numpy()\n",
    "        fmap = fmap[0, :, :, :, 0] # torch.Size([64, 16, 16])\n",
    "        fmap = fmap.reshape(fmap.shape[0], fmap.shape[1]*fmap.shape[2])\n",
    "\n",
    "        # Replace 2 occurrences to find counterpart of RGB image as Semantic\n",
    "        sem_file   = path.replace(\"rgb\", \"semantic\", 2)\n",
    "        sem_img    = np.asarray(Image.open(sem_file).resize((16,16)))\n",
    "        print(\"sem_img.shape: \", sem_img.shape)\n",
    "        sem_pixels = sem_img.reshape(sem_img.shape[0]*sem_img.shape[1], sem_img.shape[2])\n",
    "        #unique_pixels = np.unique(sem_pixels, axis=0)\n",
    "        valid_indexes = [[np.argwhere((sem_pixels == p).all(axis=1))[0,0], get_label(p)]\n",
    "                            for p in sem_pixels\n",
    "                            if get_label(p) in VALID_OBJECTS]\n",
    "        # first value = feature index, second value = label\n",
    "        for idx in valid_indexes:\n",
    "            features.append(fmap[:, idx[0]])\n",
    "            labels.append(VALID_OBJECTS.index(idx[1]))\n",
    "\n",
    "        display(str(i) + \" / 85\")\n",
    "        i += 1\n",
    "    \n",
    "    savemat(file,{'features': np.asarray(features),\n",
    "                  'labels'  : np.asarray(labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sem_img.shape:  (16, 16, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'85 / 85'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = 'area_3_data_pano_s2cnn_dims_128_128_16_16.mat'\n",
    "save_features_and_labels(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"C:\\\\Users\\\\ustundag\\\\GitHub\\\\2D-3D-Semantics\\\\\"+file)\n",
    "features = data[\"features\"]\n",
    "labels   = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3141, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3141)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
